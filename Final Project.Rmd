
---
title: 'Which Features of the House Have the Most Influence on its Sale Price?'
subtitle: 'A Multiple Linear Regression Analysis Using R Programming Language'
author: "Fathieh Qakei"
date: "2023-12-04"
output:
  html_document: default
  word_document: default

---
\newpage






```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Load packages
library(tidyverse)
library(caret)
library(asbio)
library(olsrr)
library(xtable)
library(shiny)
library(knitr)
library(DT)
require(scatterplot3d)
require(Hmisc)
require(rgl)
require(faraway)
library(car)
library(vroom)
library(leaps)
library(corrplot)
library(ggplot2)
library(r02pro)
library(tibble)


#load data
data(sahp, package="r02pro") #sahp data set is a small version of the ames data set that is built under "modeldata" package, which originally contained 81 predictor variables and 2930 observations. sahp data set is under R package named "r02pro" that is built by Yang Feng and Jianan Zhu as a a companion package of the book "R Programming: Zero to Pro" 


display_output <- function(dataset, out_type, filter_opt = 'none') {
  
  if (out_type == "html") {
    out_table <- DT::datatable(dataset, filter = filter_opt)
  } else {
    out_table <- knitr::kable(dataset)
  } 
  
  out_table
}

# Function to calculate predicted sum of squares (PRESS)
PRESS <- function(linear.model) {
  #' calculate the predictive residuals
  pr <- residuals(linear.model)/(1-lm.influence(linear.model)$hat)
  #' calculate the PRESS
  PRESS <- sum(pr^2)
  
  return(PRESS)
}

```




## I. Introduction

### A. Study Design

The real estate market is a complex interplay of various factors that contribute to the valuation of residential properties. Understanding the dynamics of these factors is crucial for making informed decisions for both home buyers and sellers, as well as policymakers and real estate professionals. Various studies have highlighted the significance of factors such as location, property size, and structural attributes in determining property values (Chin and Chau, 2002). The search for a relatively new and relevant dataset that has sufficient sample size of observations proved challenging. However, a comprehensive and contemporary dataset analyzing house prices in Ames city-a city in Story County in the State of Iowa, United States- was found. The Ames housing dataset analyzes 80 predictor variables for 2930 properties sold in Ames, Iowa from 2006 to 2010 (De cock, 2011). Dean De Cock of Truman State University received the Ames housing dataset from the Assessorâ€™s Office. He meticulously compiled the dataset, originally containing 113 variables for 3970 property sales, to align with his teaching objectives. He focused more on 80 predictor variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) that explain the physical attributes of a house (De cock, 2011). 

To narrow down the scope of the analysis even further, a small version of the ames dataset called \(\texttt{sahp}\) that is built under **r02pro** package in R statistical software version 4.3.1 will be analyzed. \(\texttt{sahp}\) dataset is built by Yang Feng and Jianan Zhu as a a companion package of their book *R Programming: Zero to Pro* (Feng and Zhu, 2023). The dataset contains 165 observations and 12 predictor variables including the dependent or response variable, \(Y\), which is property's **sale_price** in U.S. thousand dollars. The other 10 independent or predictor variables are: **dt_sold**, the date the house is sold; **bedroom**, number of bedrooms in the house; **bathroom**, number of bathrooms; **gar_car**, size of garage in car capacity; **oa_qual**, overall material and finish quality of the house ranked as follows: 10: Very Excellent > 9: Excellent > 8: Very Good > 7: Good > 6: Above Average > 5: Average > 4: Below Average > 3: Fair > 2: Poor > 1: Very Poor; **liv_area**, living area square footage ; **lot_area**, lot size square footage; **house_style**, five styles of dwelling that include: **1Story** for One story finished, **1.5Fin** for One and one-half story: 2nd level finished, **2Story** for two story finished, **SFoyer** for split foyer, **SLvl** for split level; **kit_qual**, kitchen quality; **heat_qual**, heating quality and condition; **central_air**, central air conditioning, where N is No central air conditioning and Y is Yes (there is central air conditioning) (Feng and Zhu, 2023). Four of the predictor variables which are **house_style**, **kit_qual**, **heat_qual**, and **central_air** are categorical variables, while the rest, including the response variable, are continuous. One-hot encoding technique will be used to represent categorical variables that have more than two levels with numerical values. Each level of the **house_style** categorical variable will be represented as a binary vector. The **dt_sold** variable will be partitioned into separate columns for **day_sold**, **mo_sold**: month sold, and **year_sold**, so they are more suitable for regression modeling.


### B. Aims 
The ultimate purpose of the study is to determine which features of the house have the most significant influence on its sale price.  

## II. Methods

### A. Preliminary Model.

A multiple linear regression model is represented by the following equation:
\[ Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \beta_4 X_{i4} + \beta_5 X_{i5} + \beta_6 X_{i6} + \beta_7 X_{i7} + \beta_8 X_{i8} + \beta_9 X_{i9} + \beta_{10} X_{i{10}} + \beta_{11} X_{i{11}} + \beta_{12} X_{i{12}} + \beta_{13} X_{i{13}} + \beta_{14} X_{i{14}} + \beta_{15} X_{i{15}} + \beta_{16} X_{i{16}} + \beta_{17} X_{i{17}} + \varepsilon_i \]
 , Where:
 
\(Y_i\) is the value of the response variable in the \(i\)-th house.

\(\beta_0\) is the fixed and unknown Y-intercept parameter that represents the expected mean value of the response variable when all predictor variables are zero.

\(\beta_1, \beta_2, \ldots, \beta_{17}\) are the coefficients or the fixed and unknown slope parameters for \(X_{i1},X_{i2}, \ldots, X_{i17}\) respectively, indicating the change in the response variable for a one-unit change in each predictor variable, holding other predictors constant. 

\(\varepsilon_i\) is a random error term associated with the ith house and indicates the degree to which
\(Y_i\) remains unexplained after accounting for the model.  \( \varepsilon_i \) \(\overset{iid}{\sim}\) \(\mathcal{N}(0, \sigma^2_{\varepsilon})\), which is read as: the random error \(\varepsilon_i\)is independently and identically distributed as a normal distribution with mean 0 and variance  \(\sigma^2_{\varepsilon}\). The variance  \(\sigma^2_{\varepsilon}\) specifies how much individual values of the random variable \(\varepsilon_i\) deviate from the mean (0, in this case).

\(X_{i1} =\)  number of bedrooms for the \(i^{th}\) house, 
\(X_{i2} = \) number of bathrooms for the \(i^{th}\) house,  
\(X_{i3} =\)  size of garage in car capacity for the \(i^{th}\) house, 
\(X_{i4} = \) overall material and finish quality for the \(i^{th}\) house, 
\(X_{i5} =\)  living area square footage for the \(i^{th}\) house, 
\(X_{i6} =\)  lot area square footage for the \(i^{th}\) house, 
\(X_{i7} =\)  kitchen quality for the \(i^{th}\) house, 
\(X_{i8} =\)  heat quality and condition for the \(i^{th}\) house, 
\(X_{i9} =\)  the availability of central air conditioning for the \(i^{th}\) house, , 1 for yes and 0 for no  
\(X_{i10} =\) year the \(i^{th}\) house was sold
\(X_{i11} =\) month the \(i^{th}\) house was sold 
\(X_{i12} =\) day the \(i^{th}\) house was sold
\(X_{i13} =\) if the \(i^{th}\) house style is 1.5 story (One and one-half story: 2nd level finished), 1 for yes and 0 for no
\(X_{i14} =\) if the \(i^{th}\) house style is 1 story finished , 1 for yes and 0 for no 
\(X_{i15} =\) if the \(i^{th}\) house style is 2 story finished , 1 for yes and 0 for no 
\(X_{i16} =\) if the \(i^{th}\) house style is split foyer , 1 for yes and 0 for no 
\(X_{i17} =\) if the \(i^{th}\) house style is split level, 1 for yes and 0 for no 
                        
**Explanation of OLS Estimators**

In linear regression, the Ordinary Least Squares (OLS) method aims to find the best-fitting line through a set of data points. The OLS estimators are the values assigned to the coefficients (\(\beta_i\)) in the regression model, and they are chosen to minimize the least-squares criterion.

The least-squares criterion is defined as the sum of the squared residuals, where the residual for each observation \(i\) is the difference between the observed value (\(Y_i\)) and the predicted value (\(\hat{Y}_i\)):

\[ \text{Residual}(e_i) = Y_i - \hat{Y}_i \]

The least-squares criterion is formulated as:

\[ \text{SSE} = \sum_{i=1}^{n} (e_i)^2 \]

The goal of OLS is to find the values of the coefficients that minimize this sum across all observations. The resulting OLS estimators represent the coefficients that make the model the best fit to the observed data, striking a balance between accurately representing the data and avoiding overfitting.
The OLS estimated model can be expressed as follows:

$\hat{Y}_i = b_0 + b_1 \cdot X_{i1} + b_2 \cdot X_{i2} + b_3 \cdot X_{i3} + b_4 \cdot X_{i4} + b_5 \cdot X_{i5} + b_6 \cdot X_{i6} + b_7 \cdot X_{i7} + b_8 \cdot X_{i8} + b_9 \cdot X_{i9} + b_{10} \cdot X_{i10} + b_{11} \cdot X_{i11} + b_{12} \cdot X_{i12} + b_{13} \cdot X_{i13} + b_{14} \cdot X_{i14} + b_{15} \cdot X_{i15} + b_{16} \cdot X_{i16} + b_{17} \cdot X_{i17}$

where:

 \( \hat{Y}_i \) is the predicted or estimated value for the \(ith\) house,
 \( b_0, b_1, b_2, \ldots, b_{17} \) are the OLS coefficient estimators,
 \( X_{i1}, X_{i2}, \ldots, X_{i17} \) are same values of the independent variables for the \(ith\) house defined previously.

**OLS Assumptions**

The effectiveness of OLS relies on several key assumptions as mentioned by Valchanov in his article *Exploring the 5 OLS Assumptions for Linear Regression Analysis* (2021):

The relationship between the independent variables and the dependent variable is assumed to be linear. Additionally, each independent variable is uncorrelated with the error term. This ensures that each predictor contributes independently to the model's predictions.The error term has a population mean of zero, and observations of the error term are uncorrelated with each other. No pattern exists in the residuals that could be exploited for predicting other residuals. Also, the error term has a constant variance across all levels of the independent variables. This assumption ensures that the spread of residuals remains consistent. No independent variable is perfectly collinear with other variables. This prevents issues with estimating unique coefficients. Finally, the error term is assumed to be normally distributed for valid statistical inference.

Based on automatic variable selection methods in combination with criterion-based statistics, **house_styl**, **dt_sold**, and **central_air** variables were dropped from the model. Partial residual plots, residual-versus-fitted plots, and measures of influence were investigated and cases 79 and 105 were identified as high influence points. The assumptions of linearity, constant variance, independence, and normality of the residuals were met. Details are included in the Appendix.


### B. Final Model
The **final model** is given by:
\[ Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \beta_4 X_{i4} + \beta_5 X_{i5} + \beta_6 X_{i6} + \beta_7 X_{i7} + \beta_8 X_{i8} + \varepsilon_i \]


The estimated (fitted) multiple linear regression model is given by:

\[ \hat{Y}_i = 2.249 - 0.04332 \cdot \text{{bedroom}} + 0.04999 \cdot \text{{bathroom}} + 0.06065 \cdot \text{{gar\_car}} + 0.1305 \cdot \text{{oa\_qual}} + 0.0002139 \cdot \text{{liv\_area}} + 0.1559 \cdot \text{{lot\_area}} + 0.05123 \cdot \text{{kit\_qual}} + 0.03442 \cdot \text{{heat\_qual}} \]

where \(\hat{Y}_i\) is the predicted house price for observation \(i\), and the coefficients are based on the results obtained from summary table shown in the appendix for the final linear regression model.




## III. Results.

The hypotheses for this statistical analysis are: 

The null hypothesis (\(H_0\)) is defined as follows:
\[H_0: \beta_1 = \beta_2 = \ldots = \beta_{17} = 0\]
The null hypothesis states that none of the independent variables have significant linear relationship with the house sale price.

The corresponding alternative hypothesis (\(H_a\)) is:
\[H_a: \text{At least one } \beta_i \text{ is not equal to } 0, \text{ where } i = 1, 2, \ldots, 17\]
The alternative hypothesis asserts that at least one of the coefficients for the independent variables is not equal to zero, which indicates a significant linear relationship with the house sale price.

After conducting automatic variable selection methods, which are (in principle) based on partial F-tests, and criterion-based statistics followed by residuals diagnostics and model validation, \(\beta_{9}\) through \(\beta_{17}\) along with their corresponding \(x_i\) which are **central_air**, **year_sold**, **mo_sold**, **day_sold**, **style[, "house_style1.5Fin"]**, **style[, "house_style1Story"]**, **style[, "house_style2Story"]**, **style[, "house_styleSFoyer"]**, and **style[, "house_styleSLvl"]**did not have significant linear association with the response variable, thus dropped from the model. 

The final model is now given by: \[ Y_i = \beta_0 + \beta_1*\ \text{{bedroom}} + \beta_2*\ \text{{bathroom}} + \beta_3*\ \text{{gar_car}} +
\beta_4*\ \text{{oa_qual}} + \beta_5*\ \text{{liv_area}} + \beta_6* \ \text{{lot_area}} + \beta_7* \ \text{{kit_qual}} + \beta_8* \ \text{{heat_qual}} + \varepsilon_i \]

Finally, from implications of the Analysis of Variance (ANOVA) table for the final model , **bedroom** variable explains 1.25%  of the total variation in the house sale price,**bathroom** explains 38.12% , **gar_car** 14.68%  ,**oa_qual** 21.80%  ,**liv_area** 5.38%  ,**lot_area** 4.18%  ,**kit_qual** 0.78%  , and lastly **heat_qual** explains 0.53%  of the total variation in the house sale price. Details are shown in the Appendix. 


## IV. Discussion

It is found from analyzing \(\texttt{sahp}\) dataset that the number of bathrooms in a house has the most significant influence on its sale price, followed by its overall quality and finish material, then the garage car capacity. The rest of variables in the regression model like number of bedrooms, living area square footage , lot area square footage, kitchen quality , and heat quality has lower impact on the house sale price. While the sale date, style of the house, and the presence or absence of central air conditioning have no significant influence on sale price. The conclusion that sale date did not significantly impact the house price suggests that the real estate market may have experienced stability during that period, potentially lacking pronounced seasonality or discernible trends. Another possibility could be the relatively short time frame of the analysis, spanning from 2006 to 2010, which might not capture substantial changes in the value of residential properties. Lastly , since \(\texttt{sahp}\) dataset is a small version of the Ames housing dataset, certain insights are probably lost. 

## V. Appendix

A quick look at what the dataset looks like is printed below: 

```{r describe}
head(sahp)
```


The following summary table of the dataset shows what variables are in the dataset and their ranges.
dt_sold: The sales dates range from January 11, 2006, to July 24, 2010.
bedroom: The number of bedrooms ranges from 0 to 5.
bathroom: The number of bathrooms varies from 1 to 4.5.
gar_car: The number of garage spaces ranges from 0 to 4.
oa_qual: Overall quality ranges from 2 to 10, with one missing value.
liv_area: The living area sizes range from 438 to 3390 square feet.
lot_area: Lot area sizes vary between 1533 and 39384 square feet.
sale_price: Sale prices range from $44.0 to $545.2 thousand dollars.

These ranges provide a concise overview of the diversity and distribution of the numerical variables in the dataset. Additionally, it highlights any missing values.

```{r}

summary(sahp)
```


The dataset contains four categorical variables which are: house style, kitchen quality, heat quality,and central air. 
```{r}
str(sahp)# we see that we have four variables of character type
# Select only character type variables
char_vars <- sahp %>%
  select(where(is.character))
```


Relevant time components such as year, month, and day from the **dt_sold** variable are extracted. These components may be used as additional predictors in the model to capture seasonality or trends over different time scales. **house_style** variable is a nominal categorical variable. Thus, One-Hot encoding in R is used to convert the "house_style" column into binary dummy variables. Kitchen quality and heat quality are ordinal categorical variables that have been converted into numerical factors. Finally, Central air variable is converted to binary variable where a value of 1 represents the presence of central air and 0 represent its absence. 

```{r}
#Extract relevant time components such as year, month, and day from the `dt_sold` variable. These components may be used as additional predictors in the model to capture seasonality or trends over different time scales.
sahp <- sahp %>%
  mutate(year_sold = year(dt_sold), mo_sold = month(dt_sold), day_sold = day(dt_sold))
```


```{r}
#find unique levels of the character vectors and get their frequency tables
#for the house style
levels_house_style  <- unique(sahp$house_style)
table_house_style <- table(levels_house_style)

# Since 'house_style' is a nominal categorical variable, One-Hot encoding in R is used to convert the "house_style" column into binary dummy variables.  The -1 in the formula removes the intercept term
sahp <- within(sahp, {
  style <- model.matrix(~ house_style - 1)
})


#for the kitchen quality
levels_kitchen <- unique(sahp$kit_qual)
table_kitchen <- table(levels_kitchen)

#for the heat quality
levels_heat_quality <- unique(sahp$heat_qual)
table_heat_quality <- table(levels_heat_quality)




```


```{r}
# Convert selected character variables to unordered factors
sahp <- sahp %>%
  mutate_at(vars(kit_qual, heat_qual), as.factor)



```


```{r}
# Convert kitchen quality variable to ordered factor with specified levels
sahp$kit_qual <- factor(sahp$kit_qual, ordered = TRUE, levels = c("Fair", "Average", "Good", "Excellent"))

# Convert heat quality variable to ordered factor with specified levels
sahp$heat_qual <- factor(sahp$heat_qual, ordered = TRUE, levels = c("Fair", "Average", "Good", "Excellent"))

```


```{r}
# Convert kitchen quality variable to numeric
sahp$kit_qual <- as.numeric(sahp$kit_qual)

# Convert heat quality variable to numeric
sahp$heat_qual <- as.numeric(sahp$heat_qual)

# Convert central air variable to binary (0, 1)
# Convert "Y" to 1, "N" to 0 using ifelse
sahp$central_air <- ifelse(sahp$central_air == "Y", 1, 0)




```



There are three missing values in the \(\texttt{sahp}\) dataset : one house did not have sale price, another did not specify the overall quality of the house, and the third did not specify the size of garage in car capacity. Since median imputation of NA values is robust to outliers, NA values are replaced with the median value in their column. 

```{r}

#the total number of NA values in the sahp data set
na_values <- sum(is.na(sahp))

# There are three NA values in the `sahp` data set : one house did not have sale price, another did not specify the overall quality of the house, and the third did not specify the size of garage in car capacity or if there is even a car garage. Since median imputation of NA values is robust to outliers, NA values are replaced with the median of the column the NA value in.  
sahp$sale_price[is.na(sahp$sale_price)] <- median(sahp$sale_price, na.rm = TRUE)
sahp$oa_qual[is.na(sahp$oa_qual)] <- median(sahp$oa_qual, na.rm = TRUE)
sahp$gar_car[is.na(sahp$gar_car)] <- median(sahp$gar_car, na.rm = TRUE)






```



A probability density plot of the response variable is shown below. It indicates that the distribution of the **sale_price** data points is right-skewed. 


```{r}


# Create a probability density plot of the sale_price using ggplot2
ggplot(data.frame(x = sahp$sale_price), aes(x)) +
  geom_density(fill = "blue", color = "black", alpha = 0.5) +
  labs(title = "Probability Density Plot", x = "sale_price")

```



After logarithmic transformation of the response variable, the probability density plot is now closer to a normal distribution: 
```{r}
sahp$sale_price <- log(sahp$sale_price)

# Create a probability density plot of the sale_price after log transformation using ggplot2
ggplot(data.frame(x = sahp$sale_price), aes(x)) +
  geom_density(fill = "blue", color = "black", alpha = 0.5) +
  labs(title = "Probability Density Plot", x = "sale_price")


```


The summary table below shows the preliminary model containing all 17 predictor variables with their coefficients' estimates, standard error, t values, and probability values. The model shows that split level house style is most likely perfectly multicollinear with other predictor variables, so it is good to drop it early from the model.  


```{r}

m <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual+ central_air+ year_sold+ mo_sold+ day_sold+ style[, "house_style1.5Fin"] + style[, "house_style1Story"] +style[, "house_style2Story"]+ style [, "house_styleSFoyer"]+ style[, "house_styleSLvl"],  data = sahp)
   
summary(m) # from the summary table, "house_styleSLvl" is most likely perfectly multicollinear with other predictor variables, so it is dropped from the model. 
```


After dropping **house_styleSLvl** variable, the summary table of the model looks as following:
```{r}
m1 <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual+ central_air+ year_sold+ mo_sold+ day_sold+ style[, "house_style1.5Fin"] + style[, "house_style1Story"] +style[, "house_style2Story"]+ style [, "house_styleSFoyer"],  data = sahp)

summary(m1)
   
```

```{r}
columns_to_exclude <- c("dt_sold", "house_style")
sahp <- sahp[, !names(sahp) %in% columns_to_exclude]
```


### A. Diagnostics for Predictors.

The following scatterplot matrix indicates positive linear association between lot area and living area. 

```{r}
# Create scatterplot matrix

pairs(sahp[, 1:4])
pairs(sahp[, 5:8])
pairs(sahp[, 9:13])

```


The Pearson correlation coefficients for all pairwise association are shown in the correlation dataframe below. It shows that kitchen quality is highly positively associated with the overall material and and finish quality of the house (r=0.708)
```{r}
# Calculate the correlation matrix of the variables
as.data.frame(cor(sahp) )    


```



Strip plots for predictors and the dependent variable (jittered) are shown next to boxplots of the same data. First, it should be acknowledged that a log transformation of **sale_price** was taken. There is also skewness visible in the distributions of **lot_area**, with observations clustered below 20000 square feet and a few data points with large values. Natural-log transformation of the **lot_area** variable might be helpful. 

 
```{r}


for (i in 1:6){
  par(mfrow=c(1,2))
  stripchart(sahp[,i], main = names(sahp)[i],
                          vertical = T, method = "jitter")
  boxplot(sahp[,i], main = names(sahp)[i])
  par(mfrow=c(1,1))
}

for (i in 10:13){
  par(mfrow=c(1,2))
  stripchart(sahp[,i], main = names(sahp)[i],
                          vertical = T, method = "jitter")
  boxplot(sahp[,i], main = names(sahp)[i])
  par(mfrow=c(1,1))
}

```


Probability density plot for the **lot_area** shows apparent right skewness in its data distribution. 

```{r}
# Create a probability density plot of the lot_area after log transformation using ggplot2
ggplot(data.frame(x = sahp$lot_area), aes(x)) +
  geom_density(fill = "blue", color = "black", alpha = 0.5) +
  labs(title = "Probability Density Plot", x = "lot_area")




```



Natural-Log transformation of **lot_area** fixed the skewness and brought its distribution closer to normal.
```{r}
sahp$lot_area <- log(sahp$lot_area)

# Create a probability density plot of the lot_area after log transformation using ggplot2
ggplot(data.frame(x = sahp$lot_area), aes(x)) +
  geom_density(fill = "blue", color = "black", alpha = 0.5) +
  labs(title = "Probability Density Plot", x = "lot_area")


```


### B.  Screening of Predictors

1. Added variable plots for each of the covariates are shown. Added variable plots (also known as partial residual plots or adjusted variable plots) provide evidence of the importance of a covariate given the other covariates already in the model. They also display the nature of the relationship between the covariate and the outcome (i.e., linear, curvilinear, transformation necessary, etc.) and any problematic data points with respect to the predictor. The plots below show that **year_sold**, **mo_old**, and **day_old** covariates seem to not provide any added value to the model that already includes all other covariates; the slopes of their linear relationship with the **sale_price** outcome is close to zero. This is expected because the analysis is studying which features or physical attributes of the house explain its price the most; it is not dealing with time series data that predicts house price over time. Additionally, all styles of houses don't seem to add any value to the model that has all other covariates because the slopes of their linear relationships with the response variable appear to be close to zero..   

 
```{r}
#Added variable plots
m1 <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual+ central_air+ year_sold+ mo_sold+ day_sold+ style[, "house_style1.5Fin"] + style[, "house_style1Story"] +style[, "house_style2Story"]+ style [, "house_styleSFoyer"],  data = sahp)
prplot(m1,1)
title("Added Variable Plot for the Bedroom Covariate")

prplot(m1,2)
title("Added Variable Plot for the Bathroom Covariate")


prplot(m1,3)
title("Added Variable Plot for the gar_car Covariate")


prplot(m1,4)
title("Added Variable Plot for the oa_qual Covariate")


prplot(m1,5)
title("Added Variable Plot for the liv_area Covariate")


prplot(m1,6)
title("Added Variable Plot for the lot_area Covariate")


prplot(m1,7)
title("Added Variable Plot for the kit_qual Covariate")


prplot(m1,8)
title("Added Variable Plot for the heat_qual Covariate")


prplot(m1,9)
title("Added Variable Plot for the cental_air Covariate")


prplot(m1,10)
title("Added Variable Plot for the year_sold Covariate")


prplot(m1,11)
title("Added Variable Plot for the mo_sold Covariate")


prplot(m1,12)
title("Added Variable Plot for the day_sold Covariate")


prplot(m1,13)
title("Added Variable Plot for the house_style1.5Fin Covariate")


prplot(m1,14)
title("Added Variable Plot for the house_style1Story Covariate")


prplot(m1,15)
title("Added Variable Plot for the house_style2Story Covariate")


prplot(m1,16)
title("Added Variable Plot for the house_styleSFoyer Covariate")


```


2. Multicollinearity can create instability in estimation and so it should be avoided. **Variance inflation factors (VIF)**  measure how much the variances of the estimated regression coefficients are inflated as compared to when the predictor variables are not linearly related.  A maximum VIF in excess of 5 is a good rule of thumb for multicollinearity problems(Kutner and Nachtsheim, 2014) .A shown in the dataframe below, **house_style1Story** has VIF value of 6.038 and **house_style2Story** has VIF = 5.621, which are larger than the maximum rule of thumb and these two predictor variables may be redundant and need to be dropped from the model.  

```{r}
as.data.frame(vif(m1))

```


After dropping **house_style1Story** and **house_style2Story** from the model,  **Variance inflation factors (VIF)** for all other variables are reduced as shown below.
```{r}
m2 <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual+ central_air+ year_sold+ mo_sold+ day_sold+ style[, "house_style1.5Fin"] + style [, "house_styleSFoyer"],  data = sahp)
```


```{r}
as.data.frame(vif(m2))

```


3. **Automatic variable selection methods** will help eliminating possibly more redundant variables. It is used as a guide to the screening and removal (or addition) of predictors. Sequential replacement method is used and the following shows 8 subset regression models with the predictors included in each. The best fitting model will be chosen based on different statistical criteria followed by model validation. These criteria are \(R^2_{adj}\) (larger values are better), Bayes Information Criterion \(BIC\) (smaller values are better), and Mallow's \(C_p\) statistic (values of \(C_p\) close to \(p\) (number of beta coefficients) are better). 

The automatic variable selection methods showed that the model with 8 predictor variables which are **bedroom**, **bathroom**, **gar_car**, **oa_qual**, **liv_area**, **lot_area**, **kit_qual**, and **heat_qual**  had the largest \(R^2_{adj}\)= 0.852, smallest \(BIC\)= -277.59, and smallest \(C_p\)= 21.78. An explanation of each criterion and its results and visualization plots are shown below:



```{r}
ma <- regsubsets(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual+ central_air+ year_sold+ mo_sold+ day_sold+ style[, "house_style1.5Fin"] + style [, "house_styleSFoyer"],  data = sahp, method = "seqrep")
(sma <- summary(ma))



```


1. The adjusted coefficient of multiple determination  \(R^2_{adj}\). 


 The coefficient of multiple determination  \(R^2\) represents the proportion of the total variance in the dependent variable explained by the independent variables, \(R^2_{\text{adj}}\) adjusts this value to penalize models with a larger number of predictors, thus providing a more accurate measure of the model's goodness of fit.
Larger value represents better predictive model. Here it is the model that has 8 predictor variables which are: **bedroom**, **bathroom**, **gar_car**, **oa_qual**, **liv_area**, **lot_area**, **kit_qual**, and **heat_qual**

```{r}
sma$adjr2 #biggest value is better predicting model

```




```{r}


plot(2:9, sma$adjr2, xlab = "Number of Parameters", ylab = expression(R^2[adj]), main = "Visualizing R^2_{adj} plot")


```


2. The Bayesian Information Criterion (BIC) is another model selection method that penalizes the inclusion of additional predictors in the model. It is given by the formula: 

\[BIC = n \cdot \ln(SSE/n) + k \cdot \ln(n)\].

Smaller \(BIC\) value represents a better predictive model. Here it is the model that has 8 predictor variables which are: **bedroom**, **bathroom**, **gar_car**, **oa_qual**, **liv_area**, **lot_area**, **kit_qual**, and **heat_qual** 

```{r}
sma$bic

```




```{r}
plot(2:9, sma$bic, xlab = "Number of Parameters", ylab = expression(BIC), main = "Visualizing BIC plot")

```


3. Mallows' \(C_p\) criterion measure the total mean squared error of the fitted values for each subset regression model. It helps balance the model's goodness of fit and model simplicity . It is given by the formula:

\[ C_p = \frac{SSE_p}{MSE_{\text{full}}} - (n - 2p) \]

where \(SSE_p\) is the sum of squared errors for the model with \(p\) predictors, \(MSE_{\text{full}}\) is the mean squared error for the full model, \(n\) is the number of observations, and \(p\) is the number of predictors. \(C_p\) value that is close to \(p\) (number of beta coefficients) represents better predictive model. Here, it is the model that has 8 predictor variables which are: **bedroom**, **bathroom**, **gar_car**, **oa_qual**, **liv_area**, **lot_area**, **kit_qual**, and **heat_qual**

```{r}
sma$cp

```





```{r}
plot(2:9, sma$cp, xlab = "Number of Parameters", ylab = expression(C[p]),main = "Visualizing C_p plot")

```


Other criteria not produced by the `regsubsets` function are Akaike's information criterion \((AIC)\) and  Prediction Sum of Squares \((PRESS)\). These statistics are compared for the best two potential subset models resulted from automatic variable selection.

1.  \(AIC\) is a model selection criteria that penalize models having large numbers of predictors, and given by : The AIC_P (Corrected Akaike Information Criterion) is calculated using the formula: 
\[ \text{AIC}_P = n \ln(SSE_2) - n \ln(n) + 2p \]
The model that has smaller \(AIC\)value is a better predictive model. From the following, the model with 9 parameters is preferred. 


```{r}
# first best subset model
m3 <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual,  data = sahp)
```

```{r}
# second best subset model
m4 <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual,  data = sahp)


```

```{r}

# Extract AIC


cat("The AIC for the reduced model with 9 parameters is:", extractAIC(m3),"where 9 is the number of parameters (all predictor variables + the intercept) and -589.8394 is the AIC value", "\n") 

cat("The AIC for the full model with 8 parameters is:", extractAIC(m4),"where 8 is the number of parameters (all predictor variables + the intercept) and -584.0313 is the AIC value", "\n")
  
```


2. \(PRESS\) (Prediction Sum of Squares) assesses the predictive power of a regression model by measuring the sum of squared differences between actual and predicted responses when each observation is excluded from the model. \((PRESS)\) values are shown below. The model with 9 parameters has the lower PRESS value.

```{r}

# Extract PRESS (Predicted Error Sum of Squares, it is a validation statistic for measuring the predictive performance of the model, we need smallest value of deleted residuals)
 
PRESS(m3) #has the smallest PRESS value 
PRESS(m4)
```


# C.Model Validation

Model validation can help selecting the model that has the best predictive performance in a hold-out sample. There are several approaches to model validation, two of which are shown here.

**Leave-one-out cross validation** involves:

1. Leave out one data point and build the model using the remaining data.
2. Test the model against the data point removed in Step 1 and record the prediction error.
3. Repeat for all data points.
4. Compute the overall prediction error by averaging the prediction errors.
5. If comparing models, the model with lowest MSPE should be selected.

The MSPE is smaller for the model with 8 predictor variables. MSPE is a measure of the average squared difference between the observed values and the values predicted and given by the formula : \( MSPE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \). RMSE value that is shown below is the square root of the mean squared differences and given by \( RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} \). Lower values for MSPE or RMSE tell that the model's predictions align more with the actual data. 

```{r}

m3 <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual,  data = sahp)
m4 <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual,  data = sahp)



# Define the training method
tr <- trainControl(method="LOOCV")

# Train the first best subset model (m3)
mreduced.1 <- train(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual, data=sahp, method = "lm", trControl = tr)
print(mreduced.1)

# Train the first best subset model (m4)
mreduced.2 <- train(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual, data=sahp, method = "lm", trControl = tr)
print(mreduced.2)

```



**K-fold cross validation** is useful for large datasets where training and testing data are available/feasible. This method involves:

1. Randomly split the data into \(k\) subsets. Reserve one of the subsets for testing.
2. Build (train) the model on the remaining \(k-1\) subsets.
3. Test the model on the reserved subset and record the mean squared prediction error.
4. Repeat the process, changing the testing subset each time, until all \(k\) subsets have served as the testing set.
5. Calculate the average of the \(k\) mean squared prediction errors.
6. If comparing models, the model with the lowest MSPE should be chosen.

The MSPE is smaller for the model with 8 predictor variables

```{r}
# Define the training method
set.seed(123) 
tr <- trainControl(method = "cv", number = 10)

# Train the first best subset model (m3)
mreduced.1 <- train(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual, data=sahp, method = "lm", trControl = tr)
print(mreduced.1)

# Train the first best subset model (m4)
mreduced.2 <- train(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual, data=sahp, method = "lm", trControl = tr)
print(mreduced.2)

```



# D. Residual Diagnostics

# 1. **Model Completeness**

The histogram of residuals along with the residuals' probability density plot are shown below; they support the normality assumption of the residuals. 

```{r}
# Histogram of residuals: checks for normality 
hist(m3$residuals, col = "lightblue", main = "Histogram of Residuals") #this supports normality

#Residuals' Probability Density Plot, looks close to a normal distribution 
ggplot(data.frame(x = residuals(m3)), aes(x)) +
  geom_density(fill = "blue", color = "black", alpha = 0.5) +
  labs(title = "Residuals' Probability Density Plot", x = "residuals(m3)")

```


The fitted-versus-residual plot looks like noise with no systematic pattern, and this supports the assumption of normality and constant variance of the residuals.

```{r}
# Model looks appropriate
plot(fitted(m3), residuals(m3), main = "Fitted Vs. Residuals Plot",
     xlab = "Fitted Values", ylab = "Residuals")
```


Residuals against the response variable **sale_price** and against every predictor variable are plotted and shown below. Residuals seem to be randomly distributed with no systemic patterns.   

```{r}

# Plot residuals against sale_price
plot(sahp$sale_price, residuals(m3), main = "Residuals Against sale_price")
abline(h = 0)
lines(lowess(sahp$sale_price, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot residuals against bedroom
plot(sahp$bedroom, residuals(m3), main = "Residuals Against bedroom")
abline(h = 0)
lines(lowess(sahp$bedroom, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot residuals against bathroom
plot(sahp$bathroom, residuals(m3), main = "Residuals Against bathroom")
abline(h = 0)
lines(lowess(sahp$bathroom, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot residuals against gar_car
plot(sahp$gar_car, residuals(m3), main = "Residuals Against gar_car")
abline(h = 0)
lines(lowess(sahp$gar_car, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot residuals against oa_qual
plot(sahp$oa_qual, residuals(m3), main = "Residuals Against oa_qual")
abline(h = 0)
lines(lowess(sahp$oa_qual, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot residuals against liv_area
plot(sahp$liv_area, residuals(m3), main = "Residuals Against liv_area")
abline(h = 0)
lines(lowess(sahp$liv_area, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot residuals against lot_area
plot(sahp$lot_area, residuals(m3), main = "Residuals Against lot_area")
abline(h = 0)
lines(lowess(sahp$lot_area, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot residuals against kit_qual
plot(sahp$kit_qual, residuals(m3), main = "Residuals Against kit_qual")
abline(h = 0)
lines(lowess(sahp$kit_qual, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot residuals against heat_qual
plot(sahp$heat_qual, residuals(m3), main = "Residuals Against heat_qual")
abline(h = 0)
lines(lowess(sahp$heat_qual, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

```


Check if any variable interaction terms improve the predictive measures of the regression model. The plots below don't show any interaction between the covariates. 

```{r}

sahp$bathroom.i <- sahp$bathroom > mean(sahp$bathroom)

sahp$gar_car.i <- sahp$gar_car > mean(sahp$gar_car)

sahp$oa_qual.i <- sahp$oa_qual > mean(sahp$oa_qual)

sahp$liv_area.i <- sahp$liv_area > mean(sahp$liv_area)

sahp$lot_area.i <- log(sahp$lot_area) > mean(log(sahp$lot_area))

sahp$heat_qual.i <- sahp$heat_qual > mean(sahp$heat_qual)

sahp$central_air.i <- sahp$central_air > mean(sahp$central_air)


```


```{r}



interaction.plot(sahp$oa_qual.i,sahp$bathroom.i,sahp$sale_price )

interaction.plot(sahp$oa_qual.i,sahp$gar_car.i,sahp$sale_price )

interaction.plot(sahp$oa_qual.i,sahp$liv_area.i,sahp$sale_price )

interaction.plot(sahp$oa_qual.i,sahp$lot_area.i,sahp$sale_price )

interaction.plot(sahp$oa_qual.i,sahp$heat_qual.i,sahp$sale_price )


#testing more interaction plots out of curiosity (if any would improve the predictive measures of the regression model) 
interaction.plot(sahp$gar_car.i,sahp$lot_area.i,sahp$sale_price )

interaction.plot(sahp$gar_car.i,sahp$liv_area.i,sahp$sale_price )

interaction.plot(sahp$liv_area.i,sahp$lot_area.i,sahp$sale_price )



```


### 2. **Investigating Outliers and Influential Points**

Plots of studentized residuals, and deleted studentized residual are shown below. The assumption of constant variance (homoscedasticity) is met. The spread of residuals is roughly constant across all levels of the predicted values. Two large deleted studentized residuals are indicated which are cases 79 and 105 that are outlying with respect to the X and Y.

```{r}


# Plot studentized residuals against fitted values
plot(rstudent(m3) ~ fitted(m3), main = "Studentized Residuals vs. Fitted Values")
abline(h = 0)
lines(lowess(fitted(m3), rstudent(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Identify studentized residuals
identify(rstudent(m3) ~ fitted(m3))

# Plot deleted studentized residuals against fitted values
plot(rstandard(m3) ~ fitted(m3), main = "Deleted Studentized Residuals vs. Fitted Values")
abline(h = 0)
lines(lowess(fitted(m3), rstandard(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot all deleted studentized residuals
plot(rstandard(m3), main = "All Studentized Residuals")
abline(h = 0)
lines(lowess(rstandard(m3)), lty = 2, col = "red")

# Identify observations with absolute deleted studentized residuals greater than 3
which(abs(rstandard(m3)) > 3)  # Two unusual residuals 

```


Another way of identifying outlying cases with respect to the X and Y is the hat matrix. The hat matrix, denoted by \(H\), maps the observed values of the dependent variable to the predicted values and is defined as:

\[ H = X(X^TX)^{-1}X^T \]

where:

\(X\) is the design matrix of the model, containing the values of the predictor variables.
\(X^T\) is the transpose of the design matrix.
\((X^TX)^{-1}\) is the inverse of the product of the transpose of \(X\) and \(X\).
The diagonal elements of the hat matrix are often denoted as \(H_{ii}\), representing the leverage values.if  \(H_{ii}\) is greater than 2p/n where p: number of parameters and n: sample size, then it is a high leverage point that potentially influences the model fit (Kutner and Nachtsheim 2014). The following cases were identified as outlying with regard to their X values:


```{r}
#The hat matrix is also useful after the model has been selected and fitted for determining whether an inference for a mean response or a new observation involves a substantial extrapolation beyond the range of the data. 

#If h_new,new is well within the range of leverage values hii for the cases in the data set, no extrapolation is involved. On the other hand, if h_new,new is much larger than the leverage values for the cases in the data set, an extrapolation is indicated. 

#hat values are the diagonal elements of the hat matrix
#if hii is greater than 2p/n, then it is a high leverage point (potentially influence the model fit)
which(hatvalues(m3)> 2*9/165) # High leverage? indicate outlying cases with regard to their X values.


```



The influence of the outlying cases on the fitted values and estimated regression coefficients will be measured below by investigating the DFFITS, Cook's distance, and DFBETAS measures.
DFFITS "Difference in Fits" is a measure that considers the influence of the ith case on the fitted value \(\hat{Y}_i\) for this case. The DFFITS for the \(i\)-th observation is given by:

\[ DFFITS_i = \frac{\hat{Y}_i - \hat{Y}_{i(-i)}}{\sqrt{\text{MSE}_i \cdot h_{ii}}} \]

where:
\(\hat{Y}_i\) is the predicted value when the \(i\)-th observation is included in the model.
\(\hat{Y}_{i(-i)}\) is the predicted value when the \(i\)-th observation is excluded from the model.
\(\text{MSE}_i\) is the Mean Squared Error associated with the \(i\)-th observation.
\(h_{ii}\) is the leverage of the \(i\)-th observation.
The rule of thumb for identifying influential cases: "a case is influential if the absolute value of DFFITS exceeds 1 for small to medium data sets and 2*sqrt(p/n) for large data sets" (Kutner and Nachtsheim 2014).
The following cases were identified as influential: 

```{r}
 # DFFITS measure considers the influence of the ith case on the fitted value Y.hat for this case,
plot(dffits(m3)) # Compare to 2sqrt(p/n) 
which(abs(dffits(m3))> 2*sqrt(9/165)) # a guideline for identifying influential cases: a case is influential if the absolute value of DFFITS exceeds 1 for small to medium data sets and 2*sqrt(p/n) for large data sets.

  




```





 `DFBETAS` measures the standardized effect of deleting each individual observation on each coefficient.. For the \(k\)-th coefficient, `DFBETAS` is calculated as:

\[ \text{dfbetas}_{ik} = \frac{b_k - b_{k(i)}}{\sqrt{\text{MSE}_i^{(Ckk)}}} \]

where:
- \(b_k\) is the estimated coefficient for the \(k\)-th variable when all observations are included.
- \(b_{k(i)}\) is the estimated coefficient for the \(k\)-th variable when the \(i\)-th observation is excluded.
- \(\text{MSE}_i^{(Ckk)}\) is the Mean Squared Error associated with the \(i\)-th observation for the \(k\)-th coefficient.

The numerator represents the change in the estimated coefficient when the \(i\)-th observation is excluded, and the denominator is the square root of the Mean Squared Error for the \(k\)-th coefficient associated with the \(i\)-th observation. A good rule of thumb for identifying influential cases: "consider a case influential if the absolute value of DFBETAS exceeds 1 for small to medium data sets and 2/sqrt(n) for large data sets" (Kutner and Nachtsheim 2014) . The following orders of the DFBETAS exceed the threshold of 0.156:

```{r}
#dfbetas tell us the standardized effect of deleting each individual observation on each coefficient. The DFBETAS value by its sign indicates whether inclusion of a case leads to an increase or a decrease in the estimated regression coefficient, and its absolute magnitude shows the size of the difference relative to the estimated standard deviation of the regression coefficient. A large absolute value of DFBETAS is indicative of a large impact of the ith case on the kth regression coefficient. As a guideline for identifying influential cases, we consider a case influential if the absolute value of DFBETAS exceeds 1 for small to medium data sets and 2/sqrt(n) for large data sets. 
#check which DFBETAS are larger than the threshold of 2/sqrt(n) 
which(dfbetas(m3)> 2/sqrt(165)) #indicates the number of the dfbeta in the data frame of dfbetas.
2/sqrt(165)

```


Some of the DFBETAS for the outlying cases 79 and 105 exceed the threshold of .156; the values are shown below:  

```{r}
thresh <- 2/sqrt(165) # 0.156

#Some of the DFBETAS for cases 105 and 138 that have been identified as outlying with respect to their X values exceed the threshold of 2/sqrt(165). 
as.data.frame(dfbetas(m3)[79,]) 
as.data.frame(dfbetas(m3)[105,]) 



  
  
```




Visualize the DFBETAS for each covariate :
```{r}

dfbetas <- as.data.frame(dfbetas(m3))

#find number of observations
n <- 165

#calculate DFBETAS threshold value
thresh <- 2/sqrt(n)

#plot DFBETAS for bedroom with threshold lines
plot(dfbetas$bedroom, type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
title(main = "Visualizing DFBETAS for `bedroom` predictor variable")

#plot DFBETAS for bathroom with threshold lines
plot(dfbetas$bathroom, type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
title(main = "Visualizing DFBETAS for `bathroom` predictor variable")

#plot DFBETAS for gar_car with threshold lines 
plot(dfbetas$ gar_car, type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
title(main = "Visualizing DFBETAS for `garage car capacity` predictor variable")

#plot DFBETAS for oa_qual   with threshold lines 
plot(dfbetas$ oa_qual  , type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
title(main = "Visualizing DFBETAS for `overall house quality` predictor variable")


#plot DFBETAS for liv_area  with threshold lines 
plot(dfbetas$  liv_area  , type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
title(main = "Visualizing DFBETAS for `living area square footage` predictor variable")


#plot DFBETAS for lot_area with threshold lines 
plot(dfbetas$lot_area, type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
title(main = "Visualizing DFBETAS for `lot area square footage` predictor variable")


#plot DFBETAS for kit_qual  with threshold lines 
plot(dfbetas$kit_qual , type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
title(main = "Visualizing DFBETAS for `kitchen quality` predictor variable")

#plot DFBETAS for heat_qual with threshold lines 
plot(dfbetas$ heat_qual , type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
title(main = "Visualizing DFBETAS for `heat quality` predictor variable")


```



Finally, Cook's distance measure considers the influence of the ith case on all n fitted values. Case 105 has a  quantile that is higher than 10th percentile. Thus, case 105 might have apparent influence on the fitted values.

```{r}
 #Cook's distance measure considers the influence of the ith case on all n fitted values.

plot(cooks.distance(m3)) # Compare percentile F(p,n-p) to 10 or 20 percent. If the percentile value is less than about 10 or 20 percent, the ith case has little apparent influence on the fitted values. If, on the other hand, the percentile value is near 50 percent or more, the fitted values obtained with and without the ith case should be considered to differ substantially, implying that the ith case has a major influence on the fit of the regression function.
title(main = "Visualizing Cook's distances")
p <- pf(cooks.distance(m3),9,165-9)
q <- qf(p,9,165-9 )
which(q>.1) # case 105 has a quantile higher than 10th percentile and seem to have apparent influence on the fitted values.
which(q>.2)# no cases with quantile higher than 20th percentile seem to have apparent influence on the fitted values.


cat("Cook's distance for case 105 is:", cooks.distance(m3)[105], "\n")


```




Another ways of visualizing DFFITS, DFBETAS, and Cook's Distances:

```{r}
#Visualizing DFFITS, DFBETAS, and Cook's Distances 

ols_plot_cooksd_bar(m3) # One way to visualize Cook's distance
ols_plot_dfbetas(m3) # Visualize influence on estimation of betas
ols_plot_dffits(m3) # Visualize influence on estimation of Y

# Another approach to getting influence statistics
m2i <- influence(m3) # Save influence stats
halfnorm(cooks.distance(m3)) # Another approach to visualize Cook's distance



```



```{r}
m5 <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual, data=sahp[-c(79, 105),])

```



Model validation steps are repeated to compare a model with outliers to the one without outliers. As shown from the following two model validation steps, deleting the outliers (cases 79 and 105) improved the predictive performance of the regression model. The model without the outliers have lower MSPE value.   

```{r}
#using Leave_one_out cross validation
# Define the training method
tr <- trainControl(method="LOOCV")


#Train the model with outliers
m_with <-train(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual, data=sahp, method = "lm", trControl = tr)
print(m_with)

#Train the model without outliers
m_without <-train(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual, data=sahp[-c(79, 105), ], method = "lm", trControl = tr)
print(m_without)
```


```{r}
#using K-fold cross validation
# Define the training method
set.seed(123) 
tr <- trainControl(method = "cv", number = 10)

#Train the model with outliers
m_with <-train(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual, data=sahp, method = "lm", trControl = tr)
print(m_with)

#Train the model without outliers
m_without <-train(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual, data=sahp[-c(79, 105), ], method = "lm", trControl = tr)
print(m_without)

```



Also, from the comparison of the two summary statistics tables below, it is shown that the standard error, which is a measure of the variability of the coefficient estimates that represents the average amount by which the estimate is expected to deviate from the true population parameter, is reduced for each coefficient estimate. The final model without the outliers is said to be the best multiple linear regression model.  

```{r}
summary(m3)
summary(m5)

```                 




### 3. Constant Variance

As shown from the plot below, there are no apparent issues with non-constant variance.

```{r}
plot(abs(residuals(m5))~predict(m5), xlab = expression(hat(Y)), ylab = "Abs Residuals")
```

### 4. Normal Probability Plot 
Also, the following normal quantile plot supports the normality of the final model's residuals:

```{r}

qqnorm(residuals(m5),pch=16)
qqline(residuals(m5))

```



To answer the question of the analysis: which features of the house have the most significant influence on its sale price? The coefficient of multiple determination \(R^2\) for each predictor variable is calculated as the ratio of the regression sum of squares (SSR) to the total sum of squares (SSTO) , and it is given by the formula:

\[ R^2 = \frac{SSR}{SSTO} \]

where:
 \( SSR \) is the regression sum of squares extracted from the Analysis of Variance (ANOVA) table below for each coefficient estimate,
 \( SSTO \) is the total sum of squares that is calculated by adding the sum of squares for all coefficient estimates and sum of squares for the residuals shown in the ANOVA table below.

\( R^2 \) expresses the proportion of the total variance in the dependent variable that is explained by the independent variables in the model.

```{r}

(anova_m5 <- anova(m5))
SSTO <- sum(anova_m5$`Sum Sq`)

SSR_bedroom  <- anova_m5$`Sum Sq`[1]
R_sq_bedroom <- SSR_bedroom / SSTO

SSR_bathroom <- anova_m5$`Sum Sq`[2]
R_sq_bathroom <- SSR_bathroom / SSTO

SSR_gar_car <- anova_m5$`Sum Sq`[3]
R_sq_gar_car <- SSR_gar_car / SSTO

SSR_oa_qual <- anova_m5$`Sum Sq`[4]
R_sq_oa_qual <- SSR_oa_qual / SSTO

SSR_liv_area <- anova_m5$`Sum Sq`[5]
R_sq_liv_area <- SSR_liv_area / SSTO

SSR_lot_area <- anova_m5$`Sum Sq`[6]
R_sq_lot_area <- SSR_lot_area / SSTO

SSR_kit_qual <- anova_m5$`Sum Sq`[7]
R_sq_kit_qual <- SSR_kit_qual / SSTO 

SSR_heat_qual <- anova_m5$`Sum Sq`[8]
R_sq_heat_qual <- SSR_heat_qual / SSTO



```


The table below shows the percentage of the explained variation in house sale price by each predictor variable:

| Predictor Variable | \( R^2 \) Value |
|--------------------|-----------------|
| Bedroom            | 1.25%           |
| Bathroom           | 38.12%          |
| Gar_car            | 14.68%          |
| Oa_qual            | 21.80%          |
| Liv_area           | 5.38%           |
| Lot_area           | 4.18%           |
| Kit_qual           | 0.78%           |
| Heat_qual          | 0.53%           |




# References

\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}

<div id="refs"></div>

\hypertarget{ref-chin2002critical}{}
Chin, T. L., Chau, K. W. (2002). A Critical Review of Literature on The Hedonic Price Model. *International Journal for Housing Science and its Applications*, 27(2).

\hypertarget{ref-decock2011ames}{}
De Cock, D. (2011). Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project. *Journal of Statistics Education*, 19(3).

\hypertarget{ref-feng2022programming}{}
Feng, Y., Zhu, J. (2022). *R Programming: Zero to Pro*.

Feng, Y., Zhu, J. (2023). _r02pro: R Programming: Zero to Pro_. R package version 0.2,
<https://CRAN.R-project.org/package=r02pro>.

Kutner, M. H., Nachtsheim, C. J., Neter, J., and Li, W. (2014), Applied Linear
Statistical Models, 5th ed., McGraw-Hill Irwin.

Valchanov, I. (2021, October 20). *Exploring the 5 OLS Assumptions for Linear Regression Analysis*. 365 Data Science. <https://365datascience.com/tutorials/statistics-tutorials/ols-assumptions/>.



##  R Codes
```{r, echo=TRUE, eval=FALSE}
# Load packages
library(tidyverse)
library(caret)
library(asbio)
library(olsrr)
library(xtable)
library(shiny)
library(knitr)
library(DT)
require(scatterplot3d)
require(Hmisc)
require(rgl)
require(faraway)
library(car)
library(vroom)
library(leaps)
library(corrplot)
library(ggplot2)
library(r02pro)
library(tibble)


#load data
data(sahp, package="r02pro") #sahp data set is a small version of the ames data set that is built under "modeldata" package, which originally contained 81 predictor variables and 2930 observations. sahp data set is under R package named "r02pro" that is built by Yang Feng and Jianan Zhu as a a companion package of the book "R Programming: Zero to Pro" 


display_output <- function(dataset, out_type, filter_opt = 'none') {
  
  if (out_type == "html") {
    out_table <- DT::datatable(dataset, filter = filter_opt)
  } else {
    out_table <- knitr::kable(dataset)
  } 
  
  out_table
}

# Function to calculate predicted sum of squares (PRESS)
PRESS <- function(linear.model) {
  #' calculate the predictive residuals
  pr <- residuals(linear.model)/(1-lm.influence(linear.model)$hat)
  #' calculate the PRESS
  PRESS <- sum(pr^2)
  
  return(PRESS)
}

head(sahp)
summary(sahp)

str(sahp)# we see that we have four variables of character type
# Select only character type variables
char_vars <- sahp %>%
  select(where(is.character))

#Extract relevant time components such as year, month, and day from the `dt_sold` variable. These components may be used as additional predictors in the model to capture seasonality or trends over different time scales.
sahp <- sahp %>%
  mutate(year_sold = year(dt_sold), mo_sold = month(dt_sold), day_sold = day(dt_sold))

#find unique levels of the character vectors and get their frequency tables
#for the house style
levels_house_style  <- unique(sahp$house_style)
table_house_style <- table(levels_house_style)

# Since 'house_style' is a nominal categorical variable, One-Hot encoding in R is used to convert the "house_style" column into binary dummy variables.  The -1 in the formula removes the intercept term
sahp <- within(sahp, {
  style <- model.matrix(~ house_style - 1)
})

#for the kitchen quality
levels_kitchen <- unique(sahp$kit_qual)
table_kitchen <- table(levels_kitchen)

#for the heat quality
levels_heat_quality <- unique(sahp$heat_qual)
table_heat_quality <- table(levels_heat_quality)

# Convert selected character variables to unordered factors
sahp <- sahp %>%
  mutate_at(vars(kit_qual, heat_qual), as.factor)

# Convert kitchen quality variable to ordered factor with specified levels
sahp$kit_qual <- factor(sahp$kit_qual, ordered = TRUE, levels = c("Fair", "Average", "Good", "Excellent"))

# Convert heat quality variable to ordered factor with specified levels
sahp$heat_qual <- factor(sahp$heat_qual, ordered = TRUE, levels = c("Fair", "Average", "Good", "Excellent"))


# Convert kitchen quality variable to numeric
sahp$kit_qual <- as.numeric(sahp$kit_qual)

# Convert heat quality variable to numeric
sahp$heat_qual <- as.numeric(sahp$heat_qual)

# Convert central air variable to binary (0, 1)
# Convert "Y" to 1, "N" to 0 using ifelse
sahp$central_air <- ifelse(sahp$central_air == "Y", 1, 0)

#the total number of NA values in the sahp data set
na_values <- sum(is.na(sahp))

# There are three NA values in the `sahp` data set : one house did not have sale price, another did not specify the overall quality of the house, and the third did not specify the size of garage in car capacity or if there is even a car garage. Since median imputation of NA values is robust to outliers, NA values are replaced with the median of the column the NA value in.  
sahp$sale_price[is.na(sahp$sale_price)] <- median(sahp$sale_price, na.rm = TRUE)
sahp$oa_qual[is.na(sahp$oa_qual)] <- median(sahp$oa_qual, na.rm = TRUE)
sahp$gar_car[is.na(sahp$gar_car)] <- median(sahp$gar_car, na.rm = TRUE)

# Create a probability density plot of the sale_price using ggplot2
ggplot(data.frame(x = sahp$sale_price), aes(x)) +
  geom_density(fill = "blue", color = "black", alpha = 0.5) +
  labs(title = "Probability Density Plot", x = "sale_price")


sahp$sale_price <- log(sahp$sale_price)

# Create a probability density plot of the sale_price after log transformation using ggplot2
ggplot(data.frame(x = sahp$sale_price), aes(x)) +
  geom_density(fill = "blue", color = "black", alpha = 0.5) +
  labs(title = "Probability Density Plot", x = "sale_price")

m <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual+ central_air+ year_sold+ mo_sold+ day_sold+ style[, "house_style1.5Fin"] + style[, "house_style1Story"] +style[, "house_style2Story"]+ style [, "house_styleSFoyer"]+ style[, "house_styleSLvl"],  data = sahp)
   
summary(m) # from the summary table, "house_styleSLvl" is most likely perfectly multicollinear with other predictor variables, so it is dropped from the model. 

m1 <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual+ central_air+ year_sold+ mo_sold+ day_sold+ style[, "house_style1.5Fin"] + style[, "house_style1Story"] +style[, "house_style2Story"]+ style [, "house_styleSFoyer"],  data = sahp)

summary(m1)

columns_to_exclude <- c("dt_sold", "house_style")
sahp <- sahp[, !names(sahp) %in% columns_to_exclude]

# Create scatterplot matrix

pairs(sahp[, 1:4])
pairs(sahp[, 5:8])
pairs(sahp[, 9:13])


# Calculate the correlation matrix of the variables
as.data.frame(cor(sahp) )    



for (i in 1:6){
  par(mfrow=c(1,2))
  stripchart(sahp[,i], main = names(sahp)[i],
                          vertical = T, method = "jitter")
  boxplot(sahp[,i], main = names(sahp)[i])
  par(mfrow=c(1,1))
}

for (i in 10:13){
  par(mfrow=c(1,2))
  stripchart(sahp[,i], main = names(sahp)[i],
                          vertical = T, method = "jitter")
  boxplot(sahp[,i], main = names(sahp)[i])
  par(mfrow=c(1,1))
}

# Create a probability density plot of the lot_area after log transformation using ggplot2
ggplot(data.frame(x = sahp$lot_area), aes(x)) +
  geom_density(fill = "blue", color = "black", alpha = 0.5) +
  labs(title = "Probability Density Plot", x = "lot_area")




sahp$lot_area <- log(sahp$lot_area)

# Create a probability density plot of the lot_area after log transformation using ggplot2
ggplot(data.frame(x = sahp$lot_area), aes(x)) +
  geom_density(fill = "blue", color = "black", alpha = 0.5) +
  labs(title = "Probability Density Plot", x = "lot_area")


#Added variable plots
m1 <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual+ central_air+ year_sold+ mo_sold+ day_sold+ style[, "house_style1.5Fin"] + style[, "house_style1Story"] +style[, "house_style2Story"]+ style [, "house_styleSFoyer"],  data = sahp)
prplot(m1,1)
title("Added Variable Plot for the Bedroom Covariate")

prplot(m1,2)
title("Added Variable Plot for the Bathroom Covariate")


prplot(m1,3)
title("Added Variable Plot for the gar_car Covariate")


prplot(m1,4)
title("Added Variable Plot for the oa_qual Covariate")


prplot(m1,5)
title("Added Variable Plot for the liv_area Covariate")


prplot(m1,6)
title("Added Variable Plot for the lot_area Covariate")


prplot(m1,7)
title("Added Variable Plot for the kit_qual Covariate")


prplot(m1,8)
title("Added Variable Plot for the heat_qual Covariate")


prplot(m1,9)
title("Added Variable Plot for the cental_air Covariate")


prplot(m1,10)
title("Added Variable Plot for the year_sold Covariate")


prplot(m1,11)
title("Added Variable Plot for the mo_sold Covariate")


prplot(m1,12)
title("Added Variable Plot for the day_sold Covariate")


prplot(m1,13)
title("Added Variable Plot for the house_style1.5Fin Covariate")


prplot(m1,14)
title("Added Variable Plot for the house_style1Story Covariate")


prplot(m1,15)
title("Added Variable Plot for the house_style2Story Covariate")


prplot(m1,16)
title("Added Variable Plot for the house_styleSFoyer Covariate")


as.data.frame(vif(m1))

m2 <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual+ central_air+ year_sold+ mo_sold+ day_sold+ style[, "house_style1.5Fin"] + style [, "house_styleSFoyer"],  data = sahp)

as.data.frame(vif(m2))


ma <- regsubsets(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual+ central_air+ year_sold+ mo_sold+ day_sold+ style[, "house_style1.5Fin"] + style [, "house_styleSFoyer"],  data = sahp, method = "seqrep")
(sma <- summary(ma))

sma$adjr2 #biggest value is better predicting model


plot(2:9,sma$adjr2, xlab = "Number of Parameters", ylab = expression(R^2[adj]))

sma$bic


plot(2:9, sma$bic, xlab = "Number of Parameters", ylab = expression(BIC))


sma$cp


plot(2:9, sma$cp, xlab = "Number of Parameters", ylab = expression(C[p]))

# first best subset model
m3 <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual,  data = sahp)

# second best subset model
m4 <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual,  data = sahp)

# Extract AIC
cat("The AIC for the reduced model with 9 parameters is:", extractAIC(m3),"where 9 is the number of parameters (all predictor variables + the intercept) and -589.8394 is the AIC value", "\n") 

cat("The AIC for the full model with 8 parameters is:", extractAIC(m4),"where 8 is the number of parameters (all predictor variables + the intercept) and -584.0313 is the AIC value", "\n")
  


# Extract PRESS (Predicted Error Sum of Squares, it is a validation statistic for measuring the predictive performance of the model, we need smallest value of deleted residuals)
 
PRESS(m3) #has the smallest PRESS value 
PRESS(m4)


m3 <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual,  data = sahp)
m4 <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual,  data = sahp)


# Define the training method
tr <- trainControl(method="LOOCV")

# Train the first best subset model (m3)
mreduced.1 <- train(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual, data=sahp, method = "lm", trControl = tr)
print(mreduced.1)

# Train the first best subset model (m4)
mreduced.2 <- train(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual, data=sahp, method = "lm", trControl = tr)
print(mreduced.2)


# Define the training method
set.seed(123) 
tr <- trainControl(method = "cv", number = 10)

# Train the first best subset model (m3)
mreduced.1 <- train(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual, data=sahp, method = "lm", trControl = tr)
print(mreduced.1)

# Train the first best subset model (m4)
mreduced.2 <- train(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual, data=sahp, method = "lm", trControl = tr)
print(mreduced.2)

# Histogram of residuals: checks for normality 
hist(m3$residuals, col = "lightblue", main = "Histogram of Residuals") #this supports normality

#Residuals' Probability Density Plot, looks close to a normal distribution 
ggplot(data.frame(x = residuals(m3)), aes(x)) +
  geom_density(fill = "blue", color = "black", alpha = 0.5) +
  labs(title = "Residuals' Probability Density Plot", x = "residuals(m3)")

# Model looks appropriate
plot(fitted(m3), residuals(m3), main = "Fitted Vs. Residuals Plot",
     xlab = "Fitted Values", ylab = "Residuals")


# Plot residuals against sale_price
plot(sahp$sale_price, residuals(m3), main = "Residuals Against sale_price")
abline(h = 0)
lines(lowess(sahp$sale_price, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot residuals against bedroom
plot(sahp$bedroom, residuals(m3), main = "Residuals Against bedroom")
abline(h = 0)
lines(lowess(sahp$bedroom, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot residuals against bathroom
plot(sahp$bathroom, residuals(m3), main = "Residuals Against bathroom")
abline(h = 0)
lines(lowess(sahp$bathroom, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot residuals against gar_car
plot(sahp$gar_car, residuals(m3), main = "Residuals Against gar_car")
abline(h = 0)
lines(lowess(sahp$gar_car, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot residuals against oa_qual
plot(sahp$oa_qual, residuals(m3), main = "Residuals Against oa_qual")
abline(h = 0)
lines(lowess(sahp$oa_qual, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot residuals against liv_area
plot(sahp$liv_area, residuals(m3), main = "Residuals Against liv_area")
abline(h = 0)
lines(lowess(sahp$liv_area, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot residuals against lot_area
plot(sahp$lot_area, residuals(m3), main = "Residuals Against lot_area")
abline(h = 0)
lines(lowess(sahp$lot_area, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot residuals against kit_qual
plot(sahp$kit_qual, residuals(m3), main = "Residuals Against kit_qual")
abline(h = 0)
lines(lowess(sahp$kit_qual, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot residuals against heat_qual
plot(sahp$heat_qual, residuals(m3), main = "Residuals Against heat_qual")
abline(h = 0)
lines(lowess(sahp$heat_qual, residuals(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

sahp$bathroom.i <- sahp$bathroom > mean(sahp$bathroom)

sahp$gar_car.i <- sahp$gar_car > mean(sahp$gar_car)

sahp$oa_qual.i <- sahp$oa_qual > mean(sahp$oa_qual)

sahp$liv_area.i <- sahp$liv_area > mean(sahp$liv_area)

sahp$lot_area.i <- log(sahp$lot_area) > mean(log(sahp$lot_area))

sahp$heat_qual.i <- sahp$heat_qual > mean(sahp$heat_qual)

sahp$central_air.i <- sahp$central_air > mean(sahp$central_air)

print(sahp)

interaction.plot(sahp$oa_qual.i,sahp$bathroom.i,sahp$sale_price )

interaction.plot(sahp$oa_qual.i,sahp$gar_car.i,sahp$sale_price )

interaction.plot(sahp$oa_qual.i,sahp$liv_area.i,sahp$sale_price )

interaction.plot(sahp$oa_qual.i,sahp$lot_area.i,sahp$sale_price )

interaction.plot(sahp$oa_qual.i,sahp$heat_qual.i,sahp$sale_price )


#testing more interaction plots out of curiosity (if any would improve the predictive measures of the regression model) 
interaction.plot(sahp$gar_car.i,sahp$lot_area.i,sahp$sale_price )

interaction.plot(sahp$gar_car.i,sahp$liv_area.i,sahp$sale_price )

interaction.plot(sahp$liv_area.i,sahp$lot_area.i,sahp$sale_price )

# Plot studentized residuals against fitted values
plot(rstudent(m3) ~ fitted(m3), main = "Studentized Residuals vs. Fitted Values")
abline(h = 0)
lines(lowess(fitted(m3), rstudent(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Identify studentized residuals
identify(rstudent(m3) ~ fitted(m3))

# Plot deleted studentized residuals against fitted values
plot(rstandard(m3) ~ fitted(m3), main = "Deleted Studentized Residuals vs. Fitted Values")
abline(h = 0)
lines(lowess(fitted(m3), rstandard(m3)), lty = 2, col = "red")
legend("bottomleft", c("e=0", "LOWESS"), col = c("black", "red"), lty = c(1, 2))

# Plot all deleted studentized residuals
plot(rstandard(m3), main = "All Studentized Residuals")
abline(h = 0)
lines(lowess(rstandard(m3)), lty = 2, col = "red")

# Identify observations with absolute deleted studentized residuals greater than 3
which(abs(rstandard(m3)) > 3)  # Two unusual residuals 


#The hat matrix is also useful after the model has been selected and fitted for determining whether an inference for a mean response or a new observation involves a substantial extrapolation beyond the range of the data. 

#If h_new,new is well within the range of leverage values hii for the cases in the data set, no extrapolation is involved. On the other hand, if h_new,new is much larger than the leverage values for the cases in the data set, an extrapolation is indicated. 

#hat values are the diagonal elements of the hat matrix
#if hii is greater than 2p/n, then it is a high leverage point (potentially influence the model fit)
which(hatvalues(m3)> 2*9/165) # High leverage? indicate outlying cases with regard to their X values.


 # DFFITS measure considers the influence of the ith case on the fitted value Y.hat for this case,
plot(dffits(m3)) # Compare to 2sqrt(p/n) 
which(abs(dffits(m3))> 2*sqrt(9/165)) # a guideline for identifying influential cases: a case is influential if the absolute value of DFFITS exceeds 1 for small to medium data sets and 2*sqrt(p/n) for large data sets.

#dfbetas tell us the standardized effect of deleting each individual observation on each coefficient. The DFBETAS value by its sign indicates whether inclusion of a case leads to an increase or a decrease in the estimated regression coefficient, and its absolute magnitude shows the size of the difference relative to the estimated standard deviation of the regression coefficient. A large absolute value of DFBETAS is indicative of a large impact of the ith case on the kth regression coefficient. As a guideline for identifying influential cases, we consider a case influential if the absolute value of DFBETAS exceeds 1 for small to medium data sets and 2/sqrt(n) for large data sets. 
#check which DFBETAS are larger than the threshold of 2/sqrt(n) 
which(dfbetas(m3)> 2/sqrt(165)) #indicates the number of the dfbeta in the data frame of dfbetas.

thresh <- 2/sqrt(165) # 0.156

#Some of the DFBETAS for cases 105 and 138 that have been identified as outlying with respect to their X values exceed the threshold of 2/sqrt(165). 
as.data.frame(dfbetas(m3)[79,]) 
as.data.frame(dfbetas(m3)[105,]) 

dfbetas <- as.data.frame(dfbetas(m3))

#find number of observations
n <- 165

#calculate DFBETAS threshold value
thresh <- 2/sqrt(n)

#plot DFBETAS for bedroom with threshold lines
plot(dfbetas$bedroom, type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
title(main = "Visualizing DFBETAS for `bedroom` predictor variable")

#plot DFBETAS for bathroom with threshold lines
plot(dfbetas$bathroom, type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
title(main = "Visualizing DFBETAS for `bathroom` predictor variable")

#plot DFBETAS for gar_car with threshold lines 
plot(dfbetas$ gar_car, type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
title(main = "Visualizing DFBETAS for `garage car capacity` predictor variable")

#plot DFBETAS for oa_qual   with threshold lines 
plot(dfbetas$ oa_qual  , type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
title(main = "Visualizing DFBETAS for `overall house quality` predictor variable")


#plot DFBETAS for liv_area  with threshold lines 
plot(dfbetas$  liv_area  , type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
title(main = "Visualizing DFBETAS for `living area square footage` predictor variable")


#plot DFBETAS for lot_area with threshold lines 
plot(dfbetas$lot_area, type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
title(main = "Visualizing DFBETAS for `lot area square footage` predictor variable")


#plot DFBETAS for kit_qual  with threshold lines 
plot(dfbetas$kit_qual , type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
title(main = "Visualizing DFBETAS for `kitchen quality` predictor variable")

#plot DFBETAS for heat_qual with threshold lines 
plot(dfbetas$ heat_qual , type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
title(main = "Visualizing DFBETAS for `heat quality` predictor variable")

 #Cook's distance measure considers the influence of the ith case on all n fitted values.

plot(cooks.distance(m3)) # Compare percentile F(p,n-p) to 10 or 20 percent. If the percentile value is less than about 10 or 20 percent, the ith case has little apparent influence on the fitted values. If, on the other hand, the percentile value is near 50 percent or more, the fitted values obtained with and without the ith case should be considered to differ substantially, implying that the ith case has a major influence on the fit of the regression function.
title(main = "Visualizing Cook's distances")
p <- pf(cooks.distance(m3),9,165-9)
q <- qf(p,9,165-9 )
which(q>.1) # case 105 has a quantile higher than 10th percentile and seem to have apparent influence on the fitted values.
which(q>.2)# no cases with quantile higher than 20th percentile seem to have apparent influence on the fitted values.


cat("Cook's distance for case 105 is:", cooks.distance(m3)[105], "\n")


#Visualizing DFFITS, DFBETAS, and Cook's Distances 

ols_plot_cooksd_bar(m3) # One way to visualize Cook's distance
ols_plot_dfbetas(m3) # Visualize influence on estimation of betas
ols_plot_dffits(m3) # Visualize influence on estimation of Y

# Another approach to getting influence statistics
m2i <- influence(m3) # Save influence stats
halfnorm(cooks.distance(m3)) # Another approach to visualize Cook's distance


m5 <- lm(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual, data=sahp[-c(79, 105),])


#using Leave_one_out cross validation
# Define the training method
tr <- trainControl(method="LOOCV")


#Train the model with outliers
m_with <-train(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual, data=sahp, method = "lm", trControl = tr)
print(m_with)

#Train the model without outliers
m_without <-train(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual, data=sahp[-c(79, 105), ], method = "lm", trControl = tr)
print(m_without)

#using K-fold cross validation
# Define the training method
set.seed(123) 
tr <- trainControl(method = "cv", number = 10)

#Train the model with outliers
m_with <-train(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual, data=sahp, method = "lm", trControl = tr)
print(m_with)

#Train the model without outliers
m_without <-train(sale_price ~ bedroom+ bathroom+ gar_car+ oa_qual+ liv_area+ lot_area+ kit_qual+ heat_qual, data=sahp[-c(79, 105), ], method = "lm", trControl = tr)
print(m_without)

summary(m3)
summary(m5)


plot(abs(residuals(m5))~predict(m5), xlab = expression(hat(Y)), ylab = "Abs Residuals")


qqnorm(residuals(m5),pch=16)
qqline(residuals(m5))

anova_m5 <- anova(m5)
SSTO <- sum(anova_m5$`Sum Sq`)

  SSR_bedroom  <- anova_m5$`Sum Sq`[1]
  R_sq_bedroom <- SSR_bedroom / SSTO
  
  SSR_bathroom <- anova_m5$`Sum Sq`[2]
  R_sq_bathroom <- SSR_bathroom / SSTO
  
  SSR_gar_car <- anova_m5$`Sum Sq`[3]
  R_sq_gar_car <- SSR_gar_car / SSTO
  
  SSR_oa_qual <- anova_m5$`Sum Sq`[4]
  R_sq_oa_qual <- SSR_oa_qual / SSTO

SSR_liv_area <- anova_m5$`Sum Sq`[5]
R_sq_liv_area <- SSR_liv_area / SSTO

SSR_lot_area <- anova_m5$`Sum Sq`[6]
R_sq_lot_area <- SSR_lot_area / SSTO

SSR_kit_qual <- anova_m5$`Sum Sq`[7]
R_sq_kit_qual <- SSR_kit_qual / SSTO 

SSR_heat_qual <- anova_m5$`Sum Sq`[8]
R_sq_heat_qual <- SSR_heat_qual / SSTO

# Print SSRs
cat("\n", "Bedroom variable explains",  sprintf("%.2f%%", R_sq_bedroom * 100), " of the variation in the house sale price,")

cat("bathroom explains",  sprintf("%.2f%%", R_sq_bathroom * 100), ",")

cat("gar_car",  sprintf("%.2f%%", R_sq_gar_car * 100), " ,")

cat("oa_qual",  sprintf("%.2f%%", R_sq_oa_qual * 100), " ,")

cat("liv_area",  sprintf("%.2f%%", R_sq_liv_area * 100), " ,")

cat("lot_area",  sprintf("%.2f%%", R_sq_lot_area * 100), " ,")

cat("kit_qual",  sprintf("%.2f%%", R_sq_kit_qual * 100), " ,")

cat("and finally heat_qual explains",  sprintf("%.2f%%", R_sq_heat_qual * 100), " of the total variation in the sale price of the house.")


```



```{r}


```


```{r}


```



```{r}

```






